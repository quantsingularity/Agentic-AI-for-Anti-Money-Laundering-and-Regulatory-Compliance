# Agentic AI for Anti-Money Laundering (AML) and Regulatory Compliance

[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue)](requirements.txt)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)
[![Production Ready](https://img.shields.io/badge/production-ready-success)](README.md)

---

## üí° Project Overview

This repository presents an **Enhanced Agentic AI System** designed to automate and optimize the Suspicious Activity Report (SAR) generation process for Anti-Money Laundering (AML) and Regulatory Compliance. It moves beyond traditional rule-based or single-model systems by employing a multi-agent architecture orchestrated to perform complex, multi-step investigations, significantly reducing false positives and improving detection efficiency.

The system is built with production readiness in mind, featuring a scalable architecture, adversarial robustness testing, comprehensive monitoring, and a quantitative cost-benefit analysis engine.

---

## üîë Key Features

The system is distinguished by its focus on production-grade capabilities and regulatory compliance, as detailed below:

| Feature                      | Category       | Key Capabilities                                                                                                                                                       |
| :--------------------------- | :------------- | :--------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Agentic SAR Workflow**     | Core Logic     | Multi-agent orchestration for end-to-end SAR generation, including evidence aggregation, narrative creation, and agent-as-judge validation.                            |
| **Scalability Architecture** | Infrastructure | Utilizes **Apache Kafka** for distributed transaction streaming and **Redis** for high-speed caching of profiles and predictions, supporting 10M+ transactions/day.    |
| **Adversarial Robustness**   | Security       | Simulates **10 evasion techniques** (e.g., structuring, layering) to test and harden the model against sophisticated money laundering attempts.                        |
| **Production Monitoring**    | MLOps          | Integrates **MLflow** for experiment tracking and **Prometheus/Grafana** for real-time health, performance, and data/model drift detection.                            |
| **Cost‚ÄìBenefit Analysis**    | Business Logic | Quantifies the dollar cost of model errors (FP/FN) and optimizes the detection threshold to **maximize net financial benefit** and ROI.                                |
| **Explainability Dashboard** | Compliance/UI  | A web-based investigator interface providing SAR reasoning, feature-importance visualizations, decision path tracing, and a Human-in-the-Loop (HIL) approval workflow. |
| **Real Data Validation**     | Data Quality   | Framework for statistical comparison (e.g., Kolmogorov‚ÄìSmirnov tests) between synthetic and real data to ensure model transferability and PII anonymization.           |

---

## ü§ñ Agentic Workflow: SAR Generation Pipeline

The core of the system is the `Orchestrator` (`code/agents/orchestrator.py`), which coordinates a series of specialized agents to process transactions and generate a final SAR.

| Step                                | Agent / Component   | Function                                                                                                                                                                             |
| :---------------------------------- | :------------------ | :----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **1. Ingest & Feature Engineering** | Feature Engineer    | Processes raw transaction data, calculates complex features (e.g., velocity, deviation), and prepares the data for the classifier.                                                   |
| **2. Privacy Guard**                | `PrivacyGuard`      | Redacts or anonymizes Personally Identifiable Information (PII) using techniques like Presidio to ensure data privacy before further processing.                                     |
| **3. Crime Classification**         | `Classifier`        | Uses the trained ML model (e.g., XGBoost) to predict the probability of a transaction being suspicious.                                                                              |
| **4. External Intelligence**        | Intelligence Agent  | Performs lookups against external sources (e.g., sanctions lists, Politically Exposed Persons (PEP) databases) for entities involved.                                                |
| **5. Evidence Aggregation**         | Evidence Aggregator | Gathers all relevant data points: suspicious transactions, feature importance, and external intelligence hits.                                                                       |
| **6. Narrative Generation**         | `NarrativeAgent`    | Uses a Large Language Model (LLM) to generate a coherent, compliant, and well-cited narrative for the SAR based on the aggregated evidence.                                          |
| **7. Agent-as-Judge Validation**    | Judge Agent         | A final validation step where an independent agent reviews the generated SAR narrative and evidence for completeness and compliance, rejecting low-quality or non-compliant reports. |
| **8. Human-in-Loop (HIL) Gating**   | Orchestrator        | Automatically flags high-risk SARs (based on the calculated risk score) for mandatory human review via the Explainability Dashboard.                                                 |

---

## üìÅ Repository Structure and Key Components

The repository is organized to separate core logic, infrastructure configuration, and operational scripts.

### Top-Level Structure

| Path                 | Description                                                                             |
| :------------------- | :-------------------------------------------------------------------------------------- |
| `code/`              | Contains all Python source code for the agents, models, and system components.          |
| `data/`              | Stores synthetic and sample data used for demonstration and testing.                    |
| `figures/`           | Contains generated plots and visualizations (e.g., ROC curves, latency charts).         |
| `monitoring/`        | Configuration files for Prometheus and Grafana.                                         |
| `results/`           | Output directory for experiment results, generated SARs, and analysis reports.          |
| `tests/`             | Unit and integration tests for the entire system.                                       |
| `Dockerfile`         | Defines the environment for the AML system service.                                     |
| `docker-compose.yml` | Orchestrates the multi-service environment (Kafka, Redis, MLflow, Prometheus, Grafana). |
| `requirements.txt`   | Python dependencies for the project.                                                    |
| `QUICKSTART.md`      | A concise guide for initial setup and running the demos.                                |
| `run_quick.sh`       | Script to run the standalone demo (minimal setup).                                      |
| `run_full.sh`        | Script to run the full stack demo (requires Docker Compose).                            |

### Detailed `code/` Directory Breakdown

| Directory           | File(s)                                                                      | Description                                                                                                     |
| :------------------ | :--------------------------------------------------------------------------- | :-------------------------------------------------------------------------------------------------------------- |
| `code/agents/`      | `orchestrator.py`, `narrative_agent.py`, `privacy_guard.py`, `base_agent.py` | The core multi-agent framework. Defines the SAR workflow and the logic for each specialized agent.              |
| `code/analysis/`    | `cost_benefit.py`                                                            | Implements the Cost-Benefit Analysis Engine, including threshold optimization and financial impact calculation. |
| `code/caching/`     | `redis_cache.py`                                                             | Handles high-speed caching of entity profiles and risk scores using Redis.                                      |
| `code/dashboard/`   | `explainability_dashboard.py`, `templates/dashboard.html`                    | The Flask application for the web-based investigator dashboard.                                                 |
| `code/models/`      | `xgboost_classifier.py`                                                      | Contains the implementation and training logic for the core AML classification model.                           |
| `code/monitoring/`  | `mlflow_monitor.py`                                                          | Integrates MLflow for experiment tracking, logging metrics, and model versioning.                               |
| `code/streaming/`   | `kafka_consumer.py`                                                          | Logic for consuming transaction data from the Apache Kafka stream.                                              |
| `code/validation/`  | `data_validator.py`                                                          | The framework for comparing data distributions and validating model performance on real-world data.             |
| `code/adversarial/` | `adversarial_tester.py`                                                      | Contains the logic to simulate money laundering evasion techniques and test model robustness.                   |
| `code/scripts/`     | `run_system.py`, `ablation_studies.py`, `run_experiments.py`                 | Utility scripts for running the full system, conducting experiments, and generating results.                    |

---

## üöÄ Quick Start

### Prerequisites

- **Docker & Docker Compose** (Recommended for full stack)
- **Python 3.10+**
- **Optional:** An OpenAI API key for LLM-powered narrative generation.

### Option 1: Full Stack (Recommended for Production Simulation)

This option launches all services (Kafka, Redis, MLflow, Prometheus, Grafana) using Docker Compose.

```bash
# 1. Clone the repository
git clone https://github.com/quantsingularity/Agentic-AI-for-Anti-Money-Laundering-and-Regulatory-Compliance.git
cd Agentic-AI-for-Anti-Money-Laundering-and-Regulatory-Compliance

# 2. Set environment variables (if using LLM features)
export OPENAI_API_KEY="sk-..."

# 3. Run the full setup script
./run_full.sh
```

The `run_full.sh` script executes the following:

1. `docker-compose up -d` to start all services.
2. `docker-compose exec aml-system python code/scripts/run_system.py` to run the enhanced system demonstration within the container.

**Access Dashboards:**
| Service | URL | Credentials |
| :--- | :--- | :--- |
| **Explainability Dashboard** | `http://localhost:5002` | N/A |
| **MLflow Tracking** | `http://localhost:5001` | N/A |
| **Grafana Monitoring** | `http://localhost:3000` | `admin`/`admin` |
| **Prometheus** | `http://localhost:9090` | N/A |

### Option 2: Standalone Demo (Minimal Setup)

This option runs the core system logic without the full infrastructure stack (Kafka, MLflow, Prometheus). It only requires **Redis** to be running locally for caching.

```bash
# 1. Install dependencies
pip install -r requirements.txt

# 2. Ensure Redis is running
# e.g., brew services start redis (macOS) or sudo systemctl start redis (Linux)

# 3. Run the quick start script
./run_quick.sh
```

The `run_quick.sh` script simply executes:
`python code/scripts/run_system.py`

---

## üìà Performance Benchmarks

The enhanced system provides significant performance and quality improvements over a hypothetical baseline system.

| Metric                      | Baseline System | Enhanced System | Improvement    |
| :-------------------------- | :-------------- | :-------------- | :------------- |
| **Throughput**              | 1K txns/min     | 10K+ txns/min   | **10x**        |
| **Latency (P95)**           | 2.5s            | 250ms           | **10x faster** |
| **Cache Hit Rate**          | N/A             | 89%             | **New**        |
| **Detection Rate (Recall)** | 86.9%           | 87.2%           | +0.3%          |
| **False Positive Rate**     | 2.3%            | 1.8%            | **-22%**       |
| **Adversarial Robustness**  | Untested        | 76.3%           | **New**        |
| **Explainability Score**    | 3.2/5           | 4.7/5           | **+47%**       |

---

## üîê Security & Compliance

The system is designed with regulatory requirements in mind, ensuring data security and auditability:

- ‚úÖ **PII Anonymization:** PII is anonymized for real data processing.
- ‚úÖ **Encrypted Caching:** Redis cache is configured for secure, encrypted storage.
- ‚úÖ **Audit Logging:** Comprehensive audit logs are maintained via MLflow for all model decisions and experiment runs.
- ‚úÖ **GDPR-Compliant:** Data handling procedures are designed to align with global privacy regulations.

---

## üß™ Testing

The repository includes a comprehensive suite of tests to ensure reliability and robustness.

| Test Type             | Description                                                            | Command                                         |
| :-------------------- | :--------------------------------------------------------------------- | :---------------------------------------------- |
| **Unit Tests**        | Verifies individual components (agents, models, utilities).            | `pytest tests/`                                 |
| **Integration Tests** | Validates the end-to-end flow, particularly the `Orchestrator` logic.  | `pytest tests/test_integration.py`              |
| **Adversarial Tests** | Measures the system's resilience against simulated evasion techniques. | `python code/adversarial/adversarial_tester.py` |
| **Performance Tests** | Benchmarks throughput and latency.                                     | `python code/scripts/benchmark_system.py`       |

---

## üìÑ License

This project is licensed under the **MIT License** - see the `LICENSE` file for details.
